{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from rich import print\n",
    "from getpass import getpass\n",
    "\n",
    "oai_api_key = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Prompting\n",
    "\n",
    "We'll be using the OpenAI Python SDK. Using `gpt-3.5-turbo` & `gpt-4.1`, which has 1M input tokens interchangably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.Client(api_key=oai_api_key)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\", input=\"Write a simple email about asking for leave.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction Following Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    instructions=\"You are an employee in a small scale Indian company, where managers are quite strict. Make it very formal \\\n",
    "    and convincing and verbose.\",\n",
    "    input=\"Write a simple email about asking for leave.\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    instructions=\"You are an employee in a small scale Indian company, where managers are quite strict. Make it very formal \\\n",
    "    and convincing and verbose.\",\n",
    "    input=\"Write a simple email about asking for leave. Plan step by step and think carefully.\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    instructions=\"You are an employee in a small scale Indian company, where managers are quite strict. Make it very formal \\\n",
    "    and convincing and verbose.\",\n",
    "    input=\"Write a simple email about asking for leave. Plan step by step and think carefully.\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I want to get a json response with these how do I get it?\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "    \"subject\": \"....\",\n",
    "    \"body\": \"...\",\n",
    "    \"designation\": \"\",\n",
    "    \"contact_info\": \"\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi line prompt use \"\"\" \"\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    instructions=\"\"\"You are an employee in a small scale Indian company, where managers are quite strict. Make it very formal \\\n",
    "    and convincing and verbose. I need the data in the following json format \\\n",
    "    {\n",
    "    \"subject\": \"....\",\n",
    "    \"body\": \"...\",\n",
    "    \"designation\": \"\",\n",
    "    \"contact_info\": \"\",\n",
    "    \"age\": \"\"\n",
    "    }\"\"\",\n",
    "    input=\"Write a simple email about asking for leave.\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(output_text):\n",
    "    \"\"\"\n",
    "        Do parsing here\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter Structured Outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the other models + sdks have a similar set of prompting fundamentals. Next set of steps would be the same.\n",
    "\n",
    "Till this point, everyone used to work with the similar responses. There were issues, a lot of them:\n",
    "\n",
    "1. Prompt responses were just text\n",
    "2. You \"could\" get a json response, but even then it was very inconsistent.\n",
    "3. Building on top of text for applications becomes more difficult.\n",
    "4. Writing evalulations/test against these is also nasty work.\n",
    "\n",
    "Thankfully, all the providers now provide solutions to structured outputs (Very recent: Aug, 2024 release). Here's a [github-repo](https://github.com/imaurer/awesome-llm-json) for an overview about llms with json outputs.\n",
    "\n",
    "JSON being the lingua-franca of the web space. (Not Jason Statham)\n",
    "\n",
    "![json-statham](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTJzk-4jdjQnm7Sqi2_BPLZ2dLBEaTK51uGfQ&s)\n",
    "\n",
    "This JSON\n",
    "\n",
    "![json](https://code.visualstudio.com/assets/docs/languages/json/json_hero.png)\n",
    "\n",
    "There are 2 ways to get structured outputs:\n",
    "\n",
    "1. JSON Mode embedded in the LLM calling: OpenAI, , DeepSeek (and a few others have json mode) having varying degrees of reliability. OpenAI themselves \n",
    "2. Function calling: Newer, more powerful way to get much better outputs with tools (web search, code editor, etc).\n",
    "\n",
    "Both the above are paired with data validation/typing libraries in Python (Pydantic) and Zod (JS) to get standardized outputs. OpenAI and most other providers have their SDKs working with Pydantic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pydantic\n",
    "\n",
    "Python has internal `dataclasses` and `types` now, but still aren't enforced at runtime. To get to a place where custom types are easy to implement, Pydantic was the first library that let us do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Detour] Why do we use types in programming?\n",
    "\n",
    "- Auto-completion in the IDE (though one would argue that cursor `tabs` now fix these issues).\n",
    "- You know what is the output looking at the function definition. Easier to debug for you as well as LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_items(prices: dict[str, float]):\n",
    "    for item_name, item_price in prices.items():\n",
    "        print(item_name, item_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_items({\"apple\": 1.5, \"banana\": 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_items({\"apple\": 1.0, \"banana\": \"pratos\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from annotated_types import Gt\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitPrice(BaseModel):\n",
    "    name: str\n",
    "    price: float\n",
    "\n",
    "\n",
    "fruit1 = FruitPrice(name=\"Apple\", price=1.0)\n",
    "fruit2 = FruitPrice(name=\"Banana\", price=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fruit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit1.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_items(prices: list[FruitPrice]) -> None:\n",
    "    for item in prices:\n",
    "        print(type(item))\n",
    "        print(item.name, item.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_items([fruit1, fruit2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitPrice(BaseModel):\n",
    "    name: str\n",
    "    price: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitPrice(BaseModel):\n",
    "    name: str\n",
    "    price: Annotated[float, Gt(1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_items(prices: list[FruitPrice]) -> None:\n",
    "    for item in prices:\n",
    "        print(item.name, item.price)\n",
    "\n",
    "\n",
    "fruit1 = FruitPrice(name=\"Apple\", price=0.9)\n",
    "fruit2 = FruitPrice(name=\"Banana\", price=2.0)\n",
    "process_items([fruit1, fruit2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can increase the complexity of the model by adding more fields or combining two different types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from enum import Enum\n",
    "\n",
    "class FruitColor(Enum):\n",
    "    RED = \"red\"\n",
    "    GREEN = \"green\"\n",
    "\n",
    "class FruitTasteMeter(BaseModel):\n",
    "    sweetness: float\n",
    "    sourness: float\n",
    "    bitterness: float\n",
    "\n",
    "class FruitInfo(BaseModel):\n",
    "    name: str\n",
    "    price: float\n",
    "    color: FruitColor\n",
    "    weight: Annotated[float, Gt(0)]\n",
    "    taste: FruitTasteMeter\n",
    "\n",
    "fruit1 = FruitInfo(name=\"Apple\", price=0.9, color=FruitColor.RED, weight=1.0, \\\n",
    "    taste=FruitTasteMeter(sweetness=0.9, sourness=0.1, bitterness=0.0))\n",
    "\n",
    "print(fruit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have validations like these, various flavours of type validation.\n",
    "\n",
    "You can look at pydantic's documentation -> [click here](https://docs.pydantic.dev/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "from typing import Annotated\n",
    "import instructor\n",
    "\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    age: int\n",
    "\n",
    "    @field_validator(\"email\")\n",
    "    @classmethod\n",
    "    def validate_email(cls, v):\n",
    "        if \"@\" not in v:\n",
    "            raise ValueError(\"Invalid email format\")\n",
    "        return v.lower()\n",
    "\n",
    "    @field_validator(\"age\")\n",
    "    @classmethod\n",
    "    def validate_age(cls, v):\n",
    "        if v < 0 or v > 150:\n",
    "            raise ValueError(\"Age must be between 0 and 150\")\n",
    "        return v\n",
    "\n",
    "\n",
    "# Test the validators\n",
    "try:\n",
    "    user = UserProfile(name=\"John\", email=\"JOHN@EXAMPLE.COM\", age=-5)\n",
    "    print(f\"Valid user: {user}\")\n",
    "    print(f\"Email normalized: {user.email}\")\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These nice little properties can be leveraged in getting structured outputs with `OpenAI`.\n",
    "\n",
    "- Below is an example using the JSON Mode. I've copy pasted content from this [website page](https://fbref.com/en/players/bc7dc64d/Bukayo-Saka)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = \"\"\"Statistic\tPer 90\tPercentile\n",
    "Non-Penalty Goals\t0.28\t57  \n",
    "npxG: Non-Penalty xG\t0.31\t77  \n",
    "Shots Total\t4.22\t99  \n",
    "Assists\t0.37\t89  \n",
    "xAG: Exp. Assisted Goals\t0.38\t96  \n",
    "npxG + xAG\t0.69\t92  \n",
    "Shot-Creating Actions\t5.58\t93  \n",
    "Passes Attempted\t46.94\t83  \n",
    "Pass Completion %\t75.1%\t47  \n",
    "Progressive Passes\t4.89\t75  \n",
    "Progressive Carries\t5.78\t96  \n",
    "Successful Take-Ons\t4.77\t99  \n",
    "Touches (Att Pen)\t7.02\t92  \n",
    "Progressive Passes Rec\t13.84\t97\n",
    "Tackles\t1.35\t57  \n",
    "Interceptions\t0.46\t57  \n",
    "Blocks\t0.53\t14  \n",
    "Clearances\t0.11\t1  \n",
    "Aerials Won\t0.05\t1  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackingAttributes(BaseModel):\n",
    "    total_goals: float\n",
    "    passes_attempted: float\n",
    "    passes_completed: float\n",
    "    pass_completion_percentage: float\n",
    "    xAG: float\n",
    "    xG: float\n",
    "    takeons: float\n",
    "\n",
    "\n",
    "class DefensiveAttributes(BaseModel):\n",
    "    tackles: float\n",
    "    interceptions: float\n",
    "    blocks: float\n",
    "    clearances: float\n",
    "    aerials_won: float\n",
    "\n",
    "\n",
    "class PlayerAttributes(BaseModel):\n",
    "    attacking: AttackingAttributes\n",
    "    defensive: DefensiveAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pydantic also gives a nifty json schema that can be used with JSON Mode in OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlayerAttributes.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = PlayerAttributes.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are an expert at structured data extraction. \\\n",
    "You will be given unstructured text from a page and should convert it into the given structure.\n",
    "\n",
    "JSON schema:\n",
    "{json_schema}\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{user_content}\"},\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Statistic                 | Per 90 | Percentile |\n",
    "|---------------------------|--------|------------|\n",
    "| Non-Penalty Goals         | 0.28   | 57         |\n",
    "| npxG: Non-Penalty xG      | 0.31   | 77         |\n",
    "| Shots Total               | 4.22   | 99         |\n",
    "| Assists                   | 0.37   | 89         |\n",
    "| xAG: Exp. Assisted Goals  | 0.38   | 96         |\n",
    "| npxG + xAG                | 0.69   | 92         |\n",
    "| Shot-Creating Actions     | 5.58   | 93         |\n",
    "| Passes Attempted          | 46.94  | 83         |\n",
    "| Pass Completion %         | 75.1%  | 47         |\n",
    "| Progressive Passes        | 4.89   | 75         |\n",
    "| Progressive Carries       | 5.78   | 96         |\n",
    "| Successful Take-Ons       | 4.77   | 99         |\n",
    "| Touches (Att Pen)         | 7.02   | 92         |\n",
    "| Progressive Passes Rec    | 13.84  | 97         |\n",
    "| Tackles                   | 1.35   | 57         |\n",
    "| Interceptions             | 0.46   | 57         |\n",
    "| Blocks                    | 0.53   | 14         |\n",
    "| Clearances                | 0.11   | 1          |\n",
    "| Aerials Won               | 0.05   | 1          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty impressive, but too much hassle to get to the schema and getting the parsed content out. This was for older models. For new models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Responses API for structured outputs\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=f\"{user_content}\",\n",
    "    text_format=PlayerAttributes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.output_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response.output_parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a response format in text, but we can get it directly in the pydantic format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Responses API for structured outputs\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=f\"\"\"You are an expert at structured data extraction. \\\n",
    "You will be given unstructured text from a website and should convert it into the given structure.\"\"\",\n",
    "    input=f\"{user_content}\",\n",
    "    text_format=PlayerAttributes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.output_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic Validation Example\n",
    "\n",
    "We can add validations like below, if we manage to get no outputs for a player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, model_validator\n",
    "\n",
    "\n",
    "class PlayerAttributes(BaseModel):\n",
    "    attacking: AttackingAttributes\n",
    "    defensive: DefensiveAttributes\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def overall_checks(self):\n",
    "        total_actions = (\n",
    "            self.attacking.passes_attempted\n",
    "            + self.attacking.takeons\n",
    "            + self.defensive.tackles\n",
    "            + self.defensive.interceptions\n",
    "            + self.defensive.blocks\n",
    "            + self.defensive.clearances\n",
    "            + self.defensive.aerials_won\n",
    "        )\n",
    "\n",
    "        if total_actions == 0:\n",
    "            raise ValueError(\n",
    "                \"Player attributes imply no activity at all, check input data.\"\n",
    "            )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = \"\"\"\n",
    "Statistic\tPer 90\tPercentile\n",
    "Non-Penalty Goals\t0.00\t0\n",
    "npxG: Non-Penalty xG\t0.00\t0\n",
    "Shots Total\t0.00\t0\n",
    "Assists\t0.00\t0\n",
    "xAG: Exp. Assisted Goals\t0.00\t0\n",
    "npxG + xAG\t0.00\t0\n",
    "Shot-Creating Actions\t0.00\t0\n",
    "Passes Attempted\t0.00\t0\n",
    "Pass Completion %\t0%\t0\n",
    "Progressive Passes\t0.00\t0\n",
    "Progressive Carries\t0.00\t0\n",
    "Successful Take-Ons\t0.00\t0\n",
    "Touches (Att Pen)\t0.00\t0\n",
    "Progressive Passes Rec\t0.00\t0\n",
    "Tackles\t0.00\t0\n",
    "Interceptions\t0.00\t0\n",
    "Blocks\t0.00\t0\n",
    "Clearances\t0.00\t0\n",
    "Aerials Won\t0.00\t0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.responses.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            instructions=f\"\"\"You are an expert at structured data extraction. \\\n",
    "    You will be given unstructured text from a website and should convert it into the given structure.\"\"\",\n",
    "            input=f\"{user_content}\",\n",
    "            text_format=PlayerAttributes,\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you can see this is great! LLMs + Structured Outputs = Nice outputs that can be provided to other services. \n",
    "\n",
    "Services can be:\n",
    "\n",
    "- Another API\n",
    "- Another or the same LLM\n",
    "- To another tool\n",
    "- Pushed to a code generator (Generate React Code for UI like Loveable)\n",
    "- Directly to DB\n",
    "- To another MCP \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There's a small mistake in the code above\n",
    "\n",
    "Can you try and find it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we won't be using the original model SDK's, instead working with higher abstractions.\n",
    "\n",
    "Link to instructor -> [click here](https://python.useinstructor.com/)\n",
    "\n",
    "- One of them is `instructor`, it is based on Pydantic and steers the json output to be correct all the time. It won't exactly do everything in one go as promised, but it lets you play with multiple providers + pydantic classes magic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "ins_client = instructor.from_openai(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    name: float\n",
    "    age: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = ins_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    response_model=Person,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a dumbass\"},\n",
    "        {\"role\": \"user\", \"content\": \"John Quito is 12 years old\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: Optional[str] = None\n",
    "    age: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = ins_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=Person,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"he is 30 years old\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around the above example and try to break it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = \"\"\"Statistic\tPer 90\tPercentile\n",
    "Non-Penalty Goals\t0.28\t57  \n",
    "npxG: Non-Penalty xG\t0.31\t77  \n",
    "Shots Total\t4.22\t99  \n",
    "Assists\t0.37\t89  \n",
    "xAG: Exp. Assisted Goals\t0.38\t96  \n",
    "npxG + xAG\t0.69\t92  \n",
    "Shot-Creating Actions\t5.58\t93  \n",
    "Passes Attempted\t46.94\t83  \n",
    "Pass Completion %\t75.1%\t47  \n",
    "Progressive Passes\t4.89\t75  \n",
    "Progressive Carries\t5.78\t96  \n",
    "Successful Take-Ons\t4.77\t99  \n",
    "Touches (Att Pen)\t7.02\t92  \n",
    "Progressive Passes Rec\t13.84\t97  \n",
    "Tackles\t1.35\t57  \n",
    "Interceptions\t0.46\t57  \n",
    "Blocks\t0.53\t14  \n",
    "Clearances\t0.11\t1  \n",
    "Aerials Won\t0.05\t1  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info = ins_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PlayerAttributes,\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"{user_content}\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(player_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Statistic                 | Per 90 | Percentile |\n",
    "|---------------------------|--------|------------|\n",
    "| Non-Penalty Goals         | 0.28   | 57         |\n",
    "| npxG: Non-Penalty xG      | 0.31   | 77         |\n",
    "| Shots Total               | 4.22   | 99         |\n",
    "| Assists                   | 0.37   | 89         |\n",
    "| xAG: Exp. Assisted Goals  | 0.38   | 96         |\n",
    "| npxG + xAG                | 0.69   | 92         |\n",
    "| Shot-Creating Actions     | 5.58   | 93         |\n",
    "| Passes Attempted          | 46.94  | 83         |\n",
    "| Pass Completion %         | 75.1%  | 47         |\n",
    "| Progressive Passes        | 4.89   | 75         |\n",
    "| Progressive Carries       | 5.78   | 96         |\n",
    "| Successful Take-Ons       | 4.77   | 99         |\n",
    "| Touches (Att Pen)         | 7.02   | 92         |\n",
    "| Progressive Passes Rec    | 13.84  | 97         |\n",
    "| Tackles                   | 1.35   | 57         |\n",
    "| Interceptions             | 0.46   | 57         |\n",
    "| Blocks                    | 0.53   | 14         |\n",
    "| Clearances                | 0.11   | 1          |\n",
    "| Aerials Won               | 0.05   | 1          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class AttackingAttributes(BaseModel):\n",
    "    total_goals: float\n",
    "    passes_attempted: float\n",
    "    passes_completed: Optional[float]\n",
    "    pass_completion_percentage: float\n",
    "    xAG: float\n",
    "    xG: float\n",
    "    takeons: float\n",
    "\n",
    "\n",
    "class DefensiveAttributes(BaseModel):\n",
    "    tackles: float\n",
    "    interceptions: float\n",
    "    blocks: float\n",
    "    clearances: float\n",
    "    aerials_won: float\n",
    "\n",
    "\n",
    "class PlayerAttributes(BaseModel):\n",
    "    attacking: AttackingAttributes\n",
    "    defensive: DefensiveAttributes\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def overall_checks(self):\n",
    "        total_actions = (\n",
    "            self.attacking.passes_attempted\n",
    "            + self.attacking.takeons\n",
    "            + self.defensive.tackles\n",
    "            + self.defensive.interceptions\n",
    "            + self.defensive.blocks\n",
    "            + self.defensive.clearances\n",
    "            + self.defensive.aerials_won\n",
    "        )\n",
    "\n",
    "        if total_actions == 0:\n",
    "            raise ValueError(\n",
    "                \"Player attributes imply no activity at all, check input data.\"\n",
    "            )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info = ins_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PlayerAttributes,\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"{user_content}\"}],\n",
    ")\n",
    "print(player_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info, completion = ins_client.chat.completions.create_with_completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PlayerAttributes,\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"{user_content}\"}],\n",
    ")\n",
    "print(player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### [Detour]\n",
    "\n",
    "there's a few validators that are extended in `instructor`\n",
    "\n",
    "- `before`\n",
    "- `after`\n",
    "- `wrap`\n",
    "\n",
    "There are 2 flavours of validators:\n",
    "- `decorator` pattern\n",
    "- `Annotated` pattern\n",
    "\n",
    "Instructor uses the same patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_validator, model_validator\n",
    "\n",
    "\n",
    "class Product(BaseModel):\n",
    "    name: str\n",
    "    price: float\n",
    "    discount: float = 0.0\n",
    "    final_price: float = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def normalize_input(cls, values):\n",
    "        # Before validation - clean the input\n",
    "        if isinstance(values, dict):\n",
    "            if \"name\" in values:\n",
    "                values[\"name\"] = values[\"name\"].strip().title()\n",
    "        return values\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def calculate_final_price(self):\n",
    "        # After validation - compute derived fields\n",
    "        self.final_price = self.price * (1 - self.discount)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Test before/after validators\n",
    "product = Product(name=\"  laptop  \", price=1000, discount=0.1)\n",
    "print(f\"Product: {product.name}\")\n",
    "print(f\"Price: ${product.price}, Discount: {product.discount * 100}%\")\n",
    "print(f\"Final Price: ${product.final_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Validator example using instructor\n",
    "from instructor import llm_validator\n",
    "from pydantic import BeforeValidator, field_validator\n",
    "from typing import Annotated\n",
    "\n",
    "def normalize_confidence(value):\n",
    "    \"\"\"Convert percentage to decimal if needed\"\"\"\n",
    "    if isinstance(value, str) and value.endswith('%'):\n",
    "        return float(value.rstrip('%')) / 100\n",
    "    return float(value)\n",
    "\n",
    "class ReviewAnalysis(BaseModel):\n",
    "    sentiment: Annotated[\n",
    "        str, \n",
    "        llm_validator(\"The sentiment must be either 'positive', 'negative', or \\\n",
    "            'neutral'. Convert synonyms. Fix English spelling errors. \\\n",
    "            Positive, Negative, Neutral are the only valid values.\",\n",
    "            client=ins_client),\n",
    "    ]\n",
    "    confidence: Annotated[float, BeforeValidator(normalize_confidence),\n",
    "        llm_validator(\"The confidence must be between 0 and 1. Fix any spelling errors or convert synonyms.\",\n",
    "            client=ins_client)\n",
    "    ]\n",
    "    review_text: Annotated[\n",
    "        str,\n",
    "        llm_validator(\"The review text must be appropriate and not contain offensive language. \\\n",
    "            Clean up any inappropriate content.\",\n",
    "            client=ins_client),\n",
    "    ]\n",
    "    \n",
    "    @field_validator('confidence')\n",
    "    @classmethod\n",
    "    def validate_confidence_range(cls, v):\n",
    "        if not 0 <= v <= 1:\n",
    "            raise ValueError('Confidence must be between 0 and 1')\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = ReviewAnalysis(\n",
    "        sentiment=\"positive\",\n",
    "        confidence=\"85%\",\n",
    "        review_text=\"This product is awsome!\"\n",
    "    )\n",
    "print(f\"Review 1: {review1.sentiment}, Confidence: {review1.confidence}\")\n",
    "print(f\"Cleaned text: {review1.review_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review2 = ReviewAnalysis(\n",
    "    sentiment=\"good\",\n",
    "    confidence=0.92,\n",
    "    review_text=\"Great product, highly recommend!\",\n",
    ")\n",
    "print(f\"Review 2: {review1.sentiment}, Confidence: {review2.confidence}\")\n",
    "print(f\"Text: {review2.review_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving our Scouting Bot using a few more components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we need?\n",
    "\n",
    "- Get the player url\n",
    "- Scrape the data\n",
    "- Send it to the model to fetch the right data\n",
    "- Get the data and send it to a reasoning model to get our final answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs7_url = \"https://fbref.com/en/players/bc7dc64d/Bukayo-Saka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from firecrawl import FirecrawlApp, ScrapeOptions\n",
    "\n",
    "fc_api_key = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firecrawl\n",
    "\n",
    "- Using `https://www.firecrawl.dev/` we can scrape any content in any format that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the data (FBRef has hostile scraping rules), hence using firecrawl\n",
    "\n",
    "app = FirecrawlApp(api_key=fc_api_key)\n",
    "\n",
    "# Scrape a website:\n",
    "scrape_status = app.scrape_url(bs7_url, formats=[\"html\"])\n",
    "\n",
    "# scrape_status.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the model\n",
    "from typing import Literal, List, Optional\n",
    "from pydantic import HttpUrl, BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "class SocialMedia(Enum):\n",
    "    TWITTER: \"Twitter\"\n",
    "    INSTAGRAM: \"Instagram\"\n",
    "\n",
    "class SocialMediaInfo(BaseModel):\n",
    "    handle: str\n",
    "    sc_type: SocialMedia\n",
    "\n",
    "class PlayerPersonal(BaseModel):\n",
    "    name: Optional[str] = None\n",
    "    profile_pic: Optional[HttpUrl] = None\n",
    "    position: Optional[List[str]] = None\n",
    "    foot: Literal[\"Left\", \"Right\"] = None\n",
    "    height: Optional[str]\n",
    "    weight: Optional[str]\n",
    "    birthday: Optional[str]\n",
    "    birthplace: Optional[str]\n",
    "    national_team: Optional[str]\n",
    "    club: Optional[str]\n",
    "    national_team: Optional[str]\n",
    "    wages: Optional[str]\n",
    "    contract_expiring_on: Optional[str]\n",
    "    social_media: Optional[List[str]]\n",
    "\n",
    "class AttackingStats(BaseModel):\n",
    "    npg: float\n",
    "    npg_percentile: int\n",
    "    npxG: float\n",
    "    npxG_percentile: int\n",
    "    total_shots: float\n",
    "    total_shots_percentile: int\n",
    "    assists: float\n",
    "    assists_percentile: int\n",
    "    xAG: float\n",
    "    xAG_percentile: int\n",
    "    total_attacking_prowress: Optional[float] = Field(description=\"npXG + xAG\")\n",
    "    sca: Optional[float] = Field(description=\"Shot creating actions\")\n",
    "    sca_percentile: int\n",
    "    passes_attempted: float\n",
    "    passes_attempted_percentile: int\n",
    "    pass_completion: float\n",
    "    pass_completion_percentile: int\n",
    "    progressive_passes: float\n",
    "    progressive_passes_percentile: int\n",
    "    progressive_carries: float\n",
    "    progressive_carries_percentile: int\n",
    "    successful_takeons: float\n",
    "    successful_takeons_percentile: int\n",
    "    touches: float\n",
    "    touches_percentile: int\n",
    "\n",
    "\n",
    "class DefensiveStats(BaseModel):\n",
    "    tackles: float\n",
    "    tackles_percentile: int\n",
    "    interceptions: float\n",
    "    interceptions_percentile: int\n",
    "    blocks: float\n",
    "    blocks_percentile: int\n",
    "    clearances: float\n",
    "    clearances_percentile: int\n",
    "    aerials_won: float\n",
    "    aerials_won_percentile: int\n",
    "\n",
    "\n",
    "class SimilarPlayer(BaseModel):\n",
    "    name: str\n",
    "    url: Optional[HttpUrl]\n",
    "\n",
    "\n",
    "class PlayerInfo(BaseModel):\n",
    "    personal: PlayerPersonal\n",
    "    attacking_stats: AttackingStats\n",
    "    defensive_stats: DefensiveStats\n",
    "    similar_players: List[SimilarPlayer] = Field(\n",
    "        description=\"Comparing against Att Mid/Wingers with \\\n",
    "    tuple being Player Name and url of the player\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info, completion = ins_client.chat.completions.create_with_completion(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    response_model=PlayerInfo,\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"{scrape_status.html}\"}],\n",
    ")\n",
    "print(player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_info, completion = ins_client.chat.completions.create_with_completion(\n",
    "    model=\"gpt-4.1\",\n",
    "    response_model=PlayerInfo,\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"{scrape_status.html}\"}],\n",
    ")\n",
    "print(player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-modal LLMs (Vison + Language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receipt(BaseModel):\n",
    "    merchant: str\n",
    "    merchant_address: str\n",
    "    receipt_number: str\n",
    "    total_amount: float\n",
    "    receipt_date: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://www.docuclipper.com/wp-content/uploads/Receipt-Example-1018x1440.jpg\" \\\n",
    "    -O ./assets/receipt_1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open(\"./assets/receipt_1.jpg\")\n",
    "\n",
    "# display the image bigger\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instructor.multimodal import Image\n",
    "\n",
    "\n",
    "def read_images(image_paths: List[str]) -> Receipt:\n",
    "    \"\"\"\n",
    "    Given a list of image paths, identify the competitors in the images.\n",
    "    \"\"\"\n",
    "    receipt, completion = ins_client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=Receipt,\n",
    "        max_tokens=2048,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    \"What is in this image?\",\n",
    "                    *[Image.from_path(path) for path in image_paths],\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    print(completion.usage)\n",
    "    return receipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipt = read_images([\"./assets/receipt_1.jpg\"])\n",
    "print(receipt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we improve what we print out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Using OpenRouter keys with `instructor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "openrouter_api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=openrouter_api_key,\n",
    ")\n",
    "\n",
    "openrouter_client = instructor.from_openai(client)\n",
    "\n",
    "resp = openrouter_client.chat.completions.create(\n",
    "    model=\"mistralai/magistral-medium-2506\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Ivan is 28 years old\",\n",
    "        },\n",
    "    ],\n",
    "    response_model=User,\n",
    "    extra_body={\"provider\": {\"require_parameters\": True}},\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
